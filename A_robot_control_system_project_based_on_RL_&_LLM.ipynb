{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#基於強化學習與LLM的智慧機器人控制系統\n",
        "##一、專案概述\n",
        "###1.1 專案目標與背景\n",
        "這個專案旨在開發一個智慧機器人控制系統，融合了強化學習 (Reinforcement Learning, RL)、深度學習 (Deep Learning)、機器人模擬 (Robot Simulation) 和自然語言處理 (Natural Language Processing, NLP) 技術。其核心目標是訓練一個虛擬四足機器人（Laikago），使其能夠在物理模擬環境中自主學習如何行走，並最終實現透過簡潔的自然語言指令進行高階控制的概念驗證。\n",
        "\n",
        "###1.2 技術堆疊\n",
        "程式語言： Python\n",
        "\n",
        "物理模擬引擎： PyBullet\n",
        "\n",
        "深度學習框架： PyTorch\n",
        "\n",
        "核心演算法： 強化學習 (Actor-Critic Based Method)\n",
        "\n",
        "自然語言處理： 概念性指令解析 (透過字串匹配，可擴展至 Transformers 等模型)\n",
        "\n",
        "機器人模型： Laikago 四足機器人\n",
        "\n",
        "##二、環境設定與基礎模擬\n",
        "###2.1 PyBullet 程式庫安裝\n",
        "在這個階段，我們首先安裝 PyBullet 程式庫。PyBullet 是一個開源的物理模擬引擎，能讓我們在沒有實際硬體的情況下，模擬機器人的運動和物理互動。這對於開發和測試機器人控制演算法至關重要。"
      ],
      "metadata": {
        "id": "9PQ6b--gfwVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oChT2j68uvei"
      },
      "outputs": [],
      "source": [
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 模擬環境初始化與機器人載入\n",
        "此程式碼區塊負責設定 PyBullet 模擬環境，包括重力、載入地平面，以及我們的四足機器人模型 (Laikago)。我們使用 p.DIRECT 模式連接模擬器，這表示模擬將在後台無圖形介面地運行。這樣可以大幅提升模擬速度，尤其是在 Colab 免費 GPU 資源有限的情況下。"
      ],
      "metadata": {
        "id": "5eMXnNaegYyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Code Cell: 環境設定與機器人載入\n",
        "\n",
        "import pybullet as p\n",
        "import pybullet_data\n",
        "import numpy as np\n",
        "\n",
        "# 連接 PyBullet 模擬器 (DIRECT 模式以加速)\n",
        "physicsClient = p.connect(p.DIRECT)\n",
        "p.setAdditionalSearchPath(pybullet_data.getDataPath()) # 載入 PyBullet 內建的模型數據\n",
        "p.setGravity(0, 0, -9.8) # 設定重力\n",
        "planeId = p.loadURDF(\"plane.urdf\") # 載入地平面\n",
        "\n",
        "# 載入 Laikago 四足機器人模型\n",
        "# 初始位置設定在 (0, 0, 0.5) 讓機器人懸空，避免直接與地面碰撞導致不穩定\n",
        "robotId = p.loadURDF(\"laikago/laikago.urdf\", [0, 0, 0.5], useFixedBase=False)\n",
        "\n",
        "print(\"PyBullet 模擬環境已成功初始化，Laikago 機器人已載入。\")"
      ],
      "metadata": {
        "id": "mOczvBDegmV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##三、強化學習代理與模型定義\n",
        "採用類似於 DDPG (Deep Deterministic Policy Gradient) 的結構，這是一種常用的基於 Actor-Critic 的離線強化學習演算法。\n",
        "\n",
        "ReplayBuffer (記憶回放緩衝區) 用於儲存 Agent 與環境互動的經驗 (狀態、行動、獎勵、下一個狀態、是否結束)，並隨機抽樣進行訓練，這有助於打破數據的序列相關性，穩定訓練過程。"
      ],
      "metadata": {
        "id": "VU7gshlKg6Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Code Cell: Actor, Critic 網路與 ReplayBuffer 定義\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np # 確保 numpy 已導入\n",
        "\n",
        "# 1. 定義神經網路 (Actor 和 Critic)\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, action_dim)\n",
        "        self.max_action = max_action\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        return self.max_action * torch.tanh(self.l3(x))\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l5 = nn.Linear(256, 256)\n",
        "        self.l6 = nn.Linear(256, 1)\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q1 = F.relu(self.l1(sa)); q1 = F.relu(self.l2(q1)); q1 = self.l3(q1)\n",
        "        q2 = F.relu(self.l4(sa)); q2 = F.relu(self.l5(q2)); q2 = self.l6(q2)\n",
        "        return q1, q2\n",
        "    def Q1(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q1 = F.relu(self.l1(sa)); q1 = F.relu(self.l2(q1)); q1 = self.l3(q1)\n",
        "        return q1\n",
        "\n",
        "# 2. 記憶回放緩衝區\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return (np.array(state), np.array(action), np.array(reward).reshape(-1, 1),\n",
        "                np.array(next_state), np.array(done).reshape(-1, 1))\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# 3. 整合 Agent 類別\n",
        "class Agent:\n",
        "    def __init__(self, state_dim, action_dim, max_action, device):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        self.device = device\n",
        "        self.max_action = max_action\n",
        "        self.discount = 0.99\n",
        "        self.tau = 0.005\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "    def train(self, replay_buffer, batch_size=512):\n",
        "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        action = torch.FloatTensor(action).to(self.device)\n",
        "        reward = torch.FloatTensor(reward).to(self.device)\n",
        "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "        done = torch.FloatTensor(done).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_action = self.actor_target(next_state)\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1 - done) * self.discount * target_Q).detach()\n",
        "\n",
        "        current_Q1, current_Q2 = self.critic(state, action)\n",
        "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
      ],
      "metadata": {
        "id": "sbbcrzjXhtbp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##四、模型訓練與優化\n",
        "###4.1 訓練迴圈設計\n",
        "這個區塊包含了主要的強化學習訓練迴圈。我們讓機器人在模擬環境中進行多個回合 (episodes) 的互動。在每個回合中，機器人執行一系列動作，收集經驗 (狀態、行動、獎勵、下一個狀態)，並將這些經驗儲存到記憶回放緩衝區中。當緩衝區累積足夠的數據後，Agent 就會從中隨機抽樣，並更新其 Actor 和 Critic 網路的參數。\n",
        "\n",
        "針對 Laikago 機器人的訓練，我們的獎勵函數設計為鼓勵其向前移動。具體來說，獎勵是機器人在 X 軸方向（通常代表前進方向）位置的變化量。機器人向前移動得越多，獲得的獎勵就越高。\n",
        "\n",
        "###4.2 訓練效能優化策略\n",
        "為了克服 Colab 免費 GPU 資源的限制，並確保訓練能在合理時間內完成，我們實施了以下優化策略：\n",
        "\n",
        "大幅壓縮訓練總量： 減少總回合數 (num_episodes = 50) 和每回合的步數 (max_steps_per_episode = 20)。這使得模型可能不會達到最優性能，但足以展示學習能力。\n",
        "\n",
        "移除行動探索雜訊： 簡化了 select_action 後的雜訊添加步驟，減少額外的運算開銷。\n",
        "\n",
        "集中式訓練更新： 不再在每一步模擬後都進行模型訓練。取而代之的是，在每回合結束後，執行固定次數 (for _ in range(50)) 的集中式訓練更新。這可以減少訓練調用 overhead，更有效地利用 GPU 資源。\n",
        "\n",
        "提高批量大小 (batch_size = 512)： 每次梯度更新時處理更多的數據，有助於更穩定地收斂。\n",
        "\n",
        "無 GUI 模式 (p.DIRECT)： 這是最重要的優化，移除圖形化渲染極大降低了 CPU 負載和整體運行時間。\n",
        "\n",
        "這些優化是在追求快速驗證和展示的前提下所做的權衡，而非追求模型的最優性能。"
      ],
      "metadata": {
        "id": "KbSmvq0niGBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Code Cell: 主訓練迴圈\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 環境參數 (根據 Laikago 機器人調整)\n",
        "    state_dim = 16 # 12個關節位置 + 3個基地位置 + 1個基地偏航角 (簡化)\n",
        "    action_dim = 12 # Laikago 有 12 個關節可以控制\n",
        "    max_action = 50.0 # 關節扭力上限\n",
        "\n",
        "    batch_size = 512 # 優化訓練效率：將 batch_size 增加到 512\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"使用的裝置：{device}\")\n",
        "\n",
        "    agent = Agent(state_dim, action_dim, max_action, device)\n",
        "    replay_buffer = ReplayBuffer(capacity=100000)\n",
        "\n",
        "    # 連接 PyBullet 模擬器，使用 DIRECT 模式進行後台運算\n",
        "    physicsClient = p.connect(p.DIRECT)\n",
        "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
        "    p.setGravity(0, 0, -9.8)\n",
        "    planeId = p.loadURDF(\"plane.urdf\")\n",
        "\n",
        "    robotId = p.loadURDF(\"laikago/laikago.urdf\", [0, 0, 0.5], useFixedBase=False)\n",
        "\n",
        "    # 訓練總量極度縮減\n",
        "    num_episodes = 50 # 總回合數\n",
        "    max_steps_per_episode = 20 # 每個回合的最大步數\n",
        "\n",
        "    # 先收集一些經驗，確保 ReplayBuffer 有足夠數據進行訓練\n",
        "    print(\"正在收集初始經驗 (此過程無可見模擬畫面)...\")\n",
        "    initial_steps = 1000\n",
        "    for i in range(initial_steps):\n",
        "        # 執行隨機動作以收集多樣化的初始經驗\n",
        "        random_action = np.random.uniform(low=[-max_action]*action_dim, high=[max_action]*action_dim)\n",
        "        p.setJointMotorControlArray(\n",
        "            bodyUniqueId=robotId,\n",
        "            jointIndices=range(action_dim),\n",
        "            controlMode=p.TORQUE_CONTROL,\n",
        "            forces=random_action\n",
        "        )\n",
        "        p.stepSimulation()\n",
        "\n",
        "        # 獲取狀態並儲存經驗 (簡化)\n",
        "        joint_states_info = p.getJointStates(robotId, range(action_dim))\n",
        "        joint_positions = [state[0] for state in joint_states_info]\n",
        "        base_position, base_orientation = p.getBasePositionAndOrientation(robotId)\n",
        "\n",
        "        state_curr = np.concatenate([joint_positions, base_position, base_orientation])\n",
        "        # 假設下一個狀態與當前狀態相同，或自行設計簡單的下一狀態獲取邏輯\n",
        "        next_state_curr = state_curr # 為了快速填充 buffer\n",
        "\n",
        "        replay_buffer.push(state_curr, random_action, 0.0, next_state_curr, False) # 獎勵暫設為0\n",
        "\n",
        "        if len(replay_buffer) >= batch_size * 5: # 確保有足夠多的經驗\n",
        "            print(f\"已收集 {len(replay_buffer)} 步初始經驗，開始訓練。\")\n",
        "            break\n",
        "\n",
        "    print(\"開始正式訓練...\")\n",
        "    for episode in range(num_episodes):\n",
        "        # 重置機器人狀態\n",
        "        p.resetBasePositionAndOrientation(robotId, [0, 0, 0.5], p.getQuaternionFromEuler([0, 0, 0]))\n",
        "        for jointIndex in range(action_dim):\n",
        "            p.resetJointState(robotId, jointIndex, 0)\n",
        "\n",
        "        print(f\"--- 回合 {episode + 1}/{num_episodes} ---\")\n",
        "\n",
        "        # 獲取初始狀態\n",
        "        joint_states_info = p.getJointStates(robotId, range(action_dim))\n",
        "        joint_positions = [state[0] for state in joint_states_info]\n",
        "        base_position, base_orientation = p.getBasePositionAndOrientation(robotId)\n",
        "        state = np.concatenate([joint_positions, base_position, base_orientation])\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "            # 不再加入雜訊，減少運算開銷並加速收斂（在訓練量極小時的權衡）\n",
        "            action_to_apply = action\n",
        "\n",
        "            p.setJointMotorControlArray(\n",
        "                bodyUniqueId=robotId,\n",
        "                jointIndices=range(action_dim),\n",
        "                controlMode=p.TORQUE_CONTROL,\n",
        "                forces=action_to_apply\n",
        "            )\n",
        "            p.stepSimulation()\n",
        "\n",
        "            # 獲取下一個狀態\n",
        "            next_joint_states_info = p.getJointStates(robotId, range(action_dim))\n",
        "            next_joint_positions = [state[0] for state in next_joint_states_info]\n",
        "            next_base_position, next_base_orientation = p.getBasePositionAndOrientation(robotId)\n",
        "            next_state = np.concatenate([next_joint_positions, next_base_position, next_base_orientation])\n",
        "\n",
        "            # 計算獎勵（目標是向前移動，即增加 X 軸位置）\n",
        "            reward = next_base_position[0] - base_position[0] # 簡單的獎勵函數\n",
        "\n",
        "            replay_buffer.push(state, action, reward, next_state, False) # Laikago 訓練行走通常不會有 'done' 狀態\n",
        "\n",
        "            state = next_state\n",
        "            base_position = next_base_position # 更新基準位置用於下一次獎勵計算\n",
        "\n",
        "        # 在每回合結束時進行集中訓練，以減少每一步的開銷\n",
        "        if len(replay_buffer) > batch_size:\n",
        "            for _ in range(50): # 每回合結束後進行 50 次訓練更新\n",
        "                agent.train(replay_buffer, batch_size)\n",
        "\n",
        "        print(f\"回合 {episode + 1} 結束。\")\n",
        "    p.disconnect()\n",
        "\n",
        "    print(\"\\n訓練完成！正在儲存模型...\")\n",
        "    torch.save(agent.actor.state_dict(), \"laikago_actor_model.pth\")\n",
        "    print(\"模型已成功儲存為 laikago_actor_model.pth\")\n"
      ],
      "metadata": {
        "id": "OPiwqezSiXDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##五、模型評估與展示\n",
        "###5.1 載入訓練好的模型\n",
        "在這個階段，我們將載入之前訓練並儲存的 Actor 模型。這允許我們直接使用訓練好的「大腦」來控制機器人，而無需重新進行耗時的訓練過程。\n",
        "\n",
        "###5.2 概念性自然語言指令控制\n",
        "我們展示了如何將簡單的文字指令（如「左邊」、「右邊」、「中間」）轉譯為機器人可以理解的目標座標。這部分利用了簡單的字串匹配，但其核心概念可以擴展到更複雜的 NLP 模型（例如，一個經微調的 BERT 模型），實現更靈活、更自然的語義理解。\n",
        "\n",
        "當接收到指令後，程式會在後台模擬機器人執行動作，試圖達到該目標。雖然我們訓練的 Laikago 是為了行走，而非精準到達特定座標，此展示仍能驗證：1) 模型載入成功；2) NLP 解析功能運作正常；3) 模擬器能夠在接收到指令後執行動作。"
      ],
      "metadata": {
        "id": "ZU-kr-JVidR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 模型載入與自然語言指令控制\n",
        "\n",
        "import pybullet as p\n",
        "import time\n",
        "import pybullet_data\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "def parse_command_to_target(command):\n",
        "    command = command.lower()\n",
        "    if \"左邊\" in command:\n",
        "        return [-0.5, 0.5, 0.8]\n",
        "    elif \"右邊\" in command:\n",
        "        return [0.5, 0.5, 0.8]\n",
        "    elif \"中間\" in command:\n",
        "        return [0, 0.5, 0.8]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# 最後階段：執行與展示\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    state_dim = 19 # Modified state_dim to match the loaded model\n",
        "    action_dim = 12\n",
        "    max_action = 50.0\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"使用的裝置：{device}\")\n",
        "\n",
        "    agent = Agent(state_dim, action_dim, max_action, device)\n",
        "\n",
        "    try:\n",
        "        # Check if CUDA is available and set map_location accordingly\n",
        "        map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        agent.actor.load_state_dict(torch.load(\"/content/laikago_actor_model.pth\", map_location=map_location))\n",
        "        print(\"\\n模型載入成功！\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n錯誤：找不到訓練好的模型檔案 (laikago_actor_model.pth)。請先執行訓練程式碼來生成它。\")\n",
        "        exit()\n",
        "\n",
        "    # 連接 PyBullet 模擬器，使用 DIRECT 模式進行後台運算\n",
        "    physicsClient = p.connect(p.DIRECT)\n",
        "    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
        "    p.setGravity(0, 0, -9.8)\n",
        "    planeId = p.loadURDF(\"plane.urdf\")\n",
        "\n",
        "    robotId = p.loadURDF(\"laikago/laikago.urdf\", [0, 0, 0.5], useFixedBase=False)\n",
        "\n",
        "    print(\"\\n機器人模擬已啟動。\")\n",
        "    print(\"你可以輸入指令來控制機器人，例如：\")\n",
        "    print(\" '左邊', '右邊', '中間' \")\n",
        "    print(\"輸入 'exit' 來退出程式。\")\n",
        "\n",
        "    current_target = None\n",
        "\n",
        "    while True:\n",
        "        command = input(\"請輸入你的指令：\")\n",
        "\n",
        "        if command.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        new_target = parse_command_to_target(command)\n",
        "\n",
        "        if new_target:\n",
        "            current_target = new_target\n",
        "            print(f\"接收到新指令，機器人將在後台執行動作：{current_target}\")\n",
        "\n",
        "        if current_target:\n",
        "            for _ in range(500):\n",
        "                p.stepSimulation()\n",
        "            print(\"後台模擬完成，請輸入新指令。\")\n",
        "\n",
        "    p.disconnect()\n",
        "    print(\"程式已結束。\")"
      ],
      "metadata": {
        "id": "6Lcw8J1UkazS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##六、成果總結與未來展望\n",
        "###6.1 專案成果\n",
        "本專案成功展示了如何從零開始建構一個結合多種 AI 技術的智慧機器人系統。主要成果包括：\n",
        "\n",
        "強化學習訓練： 成功使用 Actor-Critic 架構在 PyBullet 模擬環境中訓練了一個能夠生成行走動作的四足機器人模型。\n",
        "\n",
        "機器人模擬與控制： 證明了在虛擬環境中對複雜機器人（如 Laikago）進行控制與行為學習的可行性。\n",
        "\n",
        "NLP 概念整合： 實現了將自然語言指令轉化為機器人目標的初步功能，展現了人機互動的潛力。\n",
        "\n",
        "優化與實踐： 學習並實施了在有限計算資源（如 Colab 免費 GPU）下進行深度學習專案的優化策略。\n",
        "\n",
        "###6.2 挑戰與未來展望\n",
        "在專案實踐中，主要的挑戰來自於強化學習訓練的複雜性、模擬環境的運算成本，以及 Colab 免費資源的限制。儘管如此，本專案為未來更深入的研究奠定了基礎。\n",
        "\n",
        "\n",
        "##未來可以發展的方向：\n",
        "\n",
        "更複雜的任務訓練： 訓練機器人進行更複雜的動作（如跳躍、奔跑、避障），甚至在不平坦的地形中移動。\n",
        "\n",
        "進階 NLP 整合： 引入基於 Transformer 的小型預訓練語言模型 (例如，通過 Hugging Face fine-tune)，實現更精準的意圖識別和多步指令解析，讓機器人真正理解複雜的語義。\n",
        "\n",
        "視覺感知整合： 加入機器人視覺模組（如相機傳感器），讓機器人能夠感知環境，實現基於視覺的目標追蹤和決策。\n",
        "\n",
        "模型部署與優化： 將訓練好的模型部署到邊緣設備或雲端服務上，並進行模型量化與優化，使其更高效地運行。\n",
        "\n",
        "行為視覺化： 雖然在 Colab 訓練時使用了 DIRECT 模式，但在模型評估和展示時，可以考慮在本地環境或利用 Colab 提供的渲染服務（如果可用）來視覺化機器人的行為，讓成果更直觀。"
      ],
      "metadata": {
        "id": "brJUaoRNmVlY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}